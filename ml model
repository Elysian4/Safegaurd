import os
import numpy as np
import librosa
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import zipfile
!apt-get install -y portaudio19-dev
!pip install sounddevice wavio


import sounddevice as sd
import wavio

# Unzip manually downloaded dataset
cremad_zip = "crema.zip"  # Update with actual path if different

if os.path.exists(cremad_zip):
    with zipfile.ZipFile(cremad_zip, "r") as zip_ref:
        zip_ref.extractall("cremad_data")

# Verify extraction
extracted_files = os.listdir("cremad_data")
print("Extracted files:", extracted_files)

# Update dataset path to 'AudioWAV' folder
cremad_path = os.path.join("cremad_data", "AudioWAV")
if not os.path.exists(cremad_path):
    raise FileNotFoundError("AudioWAV folder not found. Please check the extracted files.")

print("Using dataset folder:", cremad_path)

# Check if extracted folder contains audio files
audio_files = [f for f in os.listdir(cremad_path) if f.lower().endswith(('.wav', '.mp3', '.flac'))]
if not audio_files:
    raise ValueError("No audio files found in the dataset folder. Please check the extracted files.")

X, y = [], []

# Emotion mapping from filenames
emotion_map = {"SAD": "Sad", "ANG": "Angry", "DIS": "Disgusted", "FEA": "Fearful", "HAP": "Happy", "NEU": "Neutral"}

# Function to extract MFCC features
def extract_mfcc(file_path, max_pad_len=50):
    y, sr = librosa.load(file_path, sr=16000)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    pad_width = max_pad_len - mfcc.shape[1]

    if pad_width > 0:
        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
    else:
        mfcc = mfcc[:, :max_pad_len]

    return mfcc

# Process CREMA-D dataset
for file in audio_files:
    file_path = os.path.join(cremad_path, file)
    mfcc_features = extract_mfcc(file_path)
    X.append(mfcc_features)

    # Extract emotion from filename
    parts = file.split('_')
    if len(parts) > 2:
        emotion_label = emotion_map.get(parts[2], "Unknown")
    else:
        emotion_label = "Unknown"
    y.append(emotion_label)

# Encode labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)  # Convert emotion labels to integers

# Convert to NumPy arrays
X = np.array(X)
X = X.reshape(X.shape[0], 40, 50, 1)  # Reshape for CNN
y = np.array(y)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build CNN model
model = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(40, 50, 1)),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(len(label_encoder.classes_), activation='softmax')  # Multi-class classification
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.2f}")

# Save the model
model.save("voice_recognition_model.h5")

# Function to record audio and classify
def record_audio(filename="input.wav", duration=3, fs=16000):
    print("Recording...")
    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
    sd.wait()
    wavio.write(filename, audio, fs, sampwidth=2)
    print("Recording saved as", filename)
    return filename

def predict_emotion(file_path):
    mfcc_features = extract_mfcc(file_path)
    mfcc_features = mfcc_features.reshape(1, 40, 50, 1)
    prediction = model.predict(mfcc_features)
    predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])[0]
    print("Predicted Emotion:", predicted_label)
    return predicted_label

# Record and classify
if __name__ == "__main__":
    audio_file = record_audio()
    predict_emotion(audio_file)
